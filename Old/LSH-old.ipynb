{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08489670-0332-4612-a59b-311000828eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "327d70a8-af6c-494b-9e05-2edf85e679e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('minilm_mean_vectors.npz')['vectors'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6b743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\nobou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb8a0871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0 1 1 1 0][1 1 1 0 1][0 1 1 1 0][0 1 0 1 0][0 1 1 1 0][0 0 1 1 1][0 1 1 1 0][1 1 0 0 1][1 0 1 1 0][0 1 1 0 1][1 1 1 1 1][0 1 0 0 0][1 1 1 1 0][0 0 0 1 1][1 1 1 1 1][1 1 1 1 1][1 1 0 1 1][0 0 1 1 1][0 0 1 1 0][1 1 1 0 1][1 1 1 1 1][1 1 1 1 1][0 1 1 1 0][0 1 1 0 0][1 1 1 0 1][0 1 1 1 1][0 1 1 0 0][1 1 1 1 0][0 1 0 1 1][0 0 1 1 1][0 0 1 1 1][1 1 0 1 0][0 1 0 1 1][0 1 1 1 1][0 1 1 1 0][1 1 1 0 0][0 1 1 1 1][0 0 1 1 0][1 1 1 1 1][1 1 1 1 1][1 1 1 0 1][1 0 1 0 1][1 1 1 1 1][1 1 1 1 0][0 1 1 1 1][1 1 1 1 1][1 1 1 1 1][1 1 1 1 1][1 0 0 0 1][1 1 1 1 0][0 1 1 0 1][1 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 1 1 1 1][0 1 1 1 1][1 1 1 1 0][0 1 0 1 1][1 1 1 1 0][0 1 1 1 1][1 0 1 1 1][1 1 1 1 1][1 1 1 1 1][1 0 1 1 1][0 0 0 0 1][0 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 1 1 1 0][0 1 1 1 0][1 1 0 1 1][0 0 0 1 1][1 1 1 1 1][0 0 1 0 1][0 1 0 1 1][0 1 1 1 1][0 1 1 1 0][0 1 1 1 0][1 0 1 1 1][1 1 1 1 0][0 1 0 0 1][1 1 1 1 1][0 1 0 1 1][1 1 1 1 0][0 1 0 1 1][1 1 1 1 1][0 1 1 1 1][1 1 1 0 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 0][1 1 1 1 1][0 1 1 1 0][0 1 0 1 0][0 0 1 1 1][0 1 1 1 1][1 0 1 1 0][1 1 0 1 1][1 1 1 1 1][1 1 1 0 0][0 1 1 1 0][1 1 1 0 0][1 0 1 1 1][1 1 1 1 1][0 0 0 1 0][0 1 1 1 0][1 1 1 1 0][0 1 1 1 0][1 1 1 1 1][1 1 1 1 1][0 1 1 1 0][0 0 1 1 0][1 0 1 0 1][0 1 1 1 0][0 1 0 1 1][1 1 1 1 1][0 1 1 1 1][1 0 1 1 1][0 1 0 1 1][1 1 1 1 1][1 0 1 1 1][1 1 0 1 1][0 0 0 1 0][1 0 1 1 1][1 0 0 0 1][1 1 1 0 1][1 1 0 0 1][1 1 1 1 1][1 1 0 1 1][1 1 0 1 1][1 1 0 0 1][0 1 1 1 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 1][1 1 1 1 1][1 1 1 1 1][1 1 0 1 1][1 1 1 1 1][1 1 1 1 1][0 1 1 1 1][1 1 1 0 1][1 0 1 1 0][0 0 0 1 1][0 0 1 1 1][1 1 1 1 1][1 1 1 1 0][0 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 1 0 1 1][1 1 1 1 1][1 1 0 1 0][0 0 0 1 0][1 1 0 1 1][1 1 1 1 0][1 1 1 0 1][0 1 1 1 1][1 1 1 1 0][1 0 1 1 1][0 0 1 0 1][0 1 1 1 1][0 1 0 1 0][0 0 1 1 1][0 1 1 1 1][0 1 1 1 1][0 1 1 1 0][0 1 1 1 1][1 1 1 1 0][1 0 1 1 0][1 1 0 1 1][0 0 1 1 1][0 0 1 1 0][1 0 1 1 0][1 1 1 1 1][1 1 1 1 0][0 1 0 1 1][0 1 0 1 1][1 0 1 1 0][0 1 1 1 0][0 1 1 1 0][1 1 1 1 0][0 0 1 1 1][0 1 1 1 0][0 1 0 1 1][0 1 1 0 1][0 1 1 1 0][1 1 0 1 0][0 1 0 1 0][1 0 0 1 1][1 1 1 1 0][1 1 0 1 0][1 1 1 1 1][0 1 1 0 1][1 1 1 1 1][1 1 1 1 1][1 1 0 1 0][1 0 0 0 1][1 1 1 0 1][0 1 1 1 1][1 0 0 1 1][1 1 1 1 1][0 1 0 0 0][1 1 1 1 1][1 1 1 1 1][0 1 1 1 0][1 1 1 0 1][1 1 1 1 0][0 1 1 1 0][1 1 1 1 1][0 1 1 1 0][1 1 0 1 1][0 1 1 1 1][1 0 1 1 1][0 1 0 1 1][0 0 1 1 0][1 1 1 1 1][1 1 1 0 1][1 1 1 1 0][1 1 1 1 0][0 0 1 1 1][0 1 1 1 1][0 0 1 1 0][1 0 1 1 0][1 0 1 1 1][1 1 0 0 0][1 1 1 1 0][1 1 1 1 1][1 1 0 1 0][0 1 1 0 0][1 0 0 0 0][0 1 0 1 1][0 0 1 1 1][1 1 0 1 1][1 1 1 0 1][0 1 1 1 1][1 1 0 1 1][0 1 1 1 1][0 1 1 1 1][0 1 1 1 1][0 1 1 1 1][0 0 1 1 1][0 1 0 1 1][1 1 0 1 1][0 1 1 1 1][1 1 1 1 1][1 1 1 0 0][0 1 1 1 1][1 0 1 1 1][1 1 1 1 0][0 1 1 1 1][1 1 0 1 1][0 0 1 1 0][0 1 1 1 1][0 1 1 1 1][1 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 0 1 1 0][0 1 1 1 0][0 1 1 1 0][0 1 1 1 1][0 0 1 1 1][0 0 1 1 1][0 0 0 1 0][0 1 1 1 0][0 1 1 1 0][0 1 1 1 1][0 1 1 1 0][0 1 1 1 0][0 0 0 1 1][0 1 1 1 0][1 1 1 0 0][1 1 1 1 0][1 1 1 1 1][0 1 0 1 1][0 1 1 1 1][1 1 0 1 0][0 1 1 1 0][0 1 1 1 1][1 1 1 1 0][0 1 1 1 1][0 1 1 1 1][0 0 1 1 0][0 1 1 1 0][0 1 1 1 0][1 1 0 0 1][0 1 1 1 0][0 1 1 1 0][0 1 0 1 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 1][1 0 1 1 1][0 1 1 1 1][0 1 1 1 1][1 1 0 1 1][0 1 1 1 0][1 1 1 0 1][1 1 1 1 0][1 1 1 1 1][1 1 1 1 1][1 1 1 1 0][1 1 1 1 1][1 1 1 0 1][0 1 1 1 1][1 1 1 1 1][1 1 1 0 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 0][1 1 1 1 0][0 1 1 1 1][1 1 1 1 1][0 1 1 1 0][1 0 1 0 0][1 1 1 1 0][1 0 1 1 1][1 1 1 1 1][1 1 1 1 1][0 1 1 1 0][0 1 1 1 1][0 0 1 1 1][1 0 1 1 1][1 1 0 1 0][1 1 1 1 0][1 1 0 0 1][1 1 1 1 1][0 1 1 1 0][0 0 0 1 1][1 1 0 0 0][0 1 0 0 0][0 0 1 1 0][0 1 1 1 0][1 1 1 1 1][1 1 1 1 0][0 1 1 1 0][1 1 1 1 1][0 1 0 1 0][1 1 1 1 1][0 1 1 1 1][1 1 1 0 1][1 1 1 1 0][0 0 1 1 0][0 1 1 1 1][0 1 1 1 1][0 1 1 1 0][1 1 1 0 0][1 1 1 1 1][1 1 1 0 0][1 1 1 0 0][0 1 1 1 0][1 1 1 0 1][1 1 1 0 0][1 0 1 1 1][1 1 1 1 1][1 1 1 0 0][0 1 1 1 0][0 1 1 1 0][1 1 1 1 1][1 1 1 0 0][0 1 1 1 1][1 1 0 1 1][0 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 1 1 1 1][0 1 1 1 0][0 1 1 1 0][1 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 1 1 1 1][1 1 0 1 1][0 0 1 1 0][0 1 1 1 1][0 0 1 1 0][0 1 1 1 1][0 1 1 1 1][1 1 1 1 1][0 1 1 1 1][1 0 1 1 1][0 1 1 1 0][0 1 0 1 0][1 1 1 1 1][0 0 1 1 0][0 1 0 1 1][1 1 1 1 1][0 1 1 1 1][1 1 1 1 1][1 1 0 1 1][1 1 1 1 1][0 1 1 1 1][0 1 1 1 1][0 1 1 1 1][1 1 1 1 0][0 1 1 1 0][0 1 1 1 1][1 1 1 1 1][0 1 1 1 1][0 1 1 1 1][0 0 1 1 0][1 1 1 1 1][0 1 1 0 0][0 1 1 1 1][0 1 0 1 1][1 1 1 1 1][0 0 1 1 0][0 1 0 1 1][1 1 1 1 0][1 1 0 1 1][1 1 1 1 1][0 1 1 1 1][0 1 1 1 1][0 1 1 1 0][0 1 1 1 1][0 0 1 1 1][1 0 1 1 1][1 1 1 1 0][0 0 0 1 1][1 1 0 1 0][1 1 0 1 0][0 1 1 1 0][1 0 1 1 0][0 1 1 1 1][0 1 1 1 1][1 1 0 1 1][1 0 1 1 1][1 1 1 1 1][1 1 1 0 1][0 0 1 1 1][0 1 1 1 0][1 1 1 1 1][0 1 0 1 1][0 1 0 1 1][1 1 1 1 1][0 1 1 1 1][1 1 1 1 0][1 1 1 1 0][0 1 1 1 0][1 0 1 1 0][0 1 0 0 0][1 1 0 1 0][1 1 1 0 1][1 1 0 1 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 0][1 1 1 1 1][0 1 1 1 0][1 0 0 1 1][1 1 1 1 1][1 0 1 1 1][1 1 1 1 0][1 1 1 0 0][1 1 1 1 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 1][1 1 1 1 1][0 1 1 1 1][1 0 0 1 1][1 1 0 1 1][0 1 1 0 0][1 1 0 1 1][1 1 1 1 1][0 1 1 1 0][0 1 1 1 0][1 0 1 1 1][1 1 1 1 1][1 1 1 1 0][0 0 1 1 1][1 1 1 1 1][0 1 1 1 0][1 1 1 1 0][0 1 0 1 1][1 1 1 1 1][0 0 1 1 1][1 1 0 1 1][1 1 1 1 1][0 1 1 1 0][1 1 0 1 1][1 1 1 1 1][0 1 1 1 0][1 1 0 1 1][1 1 0 1 1][1 1 0 1 0][0 0 1 1 1][1 1 1 1 1][1 1 1 1 1][1 1 1 1 1][0 0 1 1 1][0 0 1 1 1][0 1 0 1 0][1 0 1 1 1][1 1 1 0 1][0 1 1 0 1][1 1 1 0 1][0 1 1 0 1][1 1 0 1 1]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeds = model.encode([\"cat\", \"cats\", \"american election\", \"dogs\"])\n",
    "hash = L2Hash(embeddings.shape[1], 1, 5)\n",
    "xs = hash.hash(embeddings)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae8d47-effe-43a0-9d50-73fe2410e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Hash:\n",
    "    def __init__(self, dim, r, nbits, seed=1):\n",
    "        self.seed = seed\n",
    "        self.nbits = nbits\n",
    "        \n",
    "        gen = np.random.RandomState(seed)\n",
    "        self.a = gen.normal(0, 1, (nbits, dim))\n",
    "        self.b = gen.uniform(0.0, r)\n",
    "        self.r = r\n",
    "\n",
    "    def hash(self, vectors):\n",
    "        normalized_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        hash_values = (np.dot(normalized_vectors, self.a.T) + self.b) / self.r\n",
    "        hash_binary = (hash_values >= 0).astype(int)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2e819-6400-4a12-8000-bb491906368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHIndex:\n",
    "    def __init__(self, indices, vectors, r=5, num_bins=20, num_projections=5, seed=1):\n",
    "        # Add indexes into vectors (these would be lost when binning)\n",
    "        indexed_vectors = np.hstack((indices[:, np.newaxis], vectors))\n",
    "        # Store the vector dimension\n",
    "        self._dim = vectors.shape[1]\n",
    "\n",
    "        # Created hash codes by applying our projections\n",
    "        self._hasher = L2Hash(self._dim, r, seed, num_projections)\n",
    "        self._r = r\n",
    "        self._num_projections = num_projections\n",
    "        self._seed = seed\n",
    "        hash_codes = self._hasher.hash(vectors)\n",
    "\n",
    "        # Create bins based on the hash codes (lowest and highest code are the boundaries)\n",
    "        self._bins = self.__create_bins(hash_codes, num_bins)\n",
    "        self._binned_vectors = self.__hashes_to_bins(indexed_vectors, hash_codes, self._bins)\n",
    "        self._num_bins = num_bins\n",
    "    \n",
    "    # LSH is probabilistic, select most common bin (or randomly if more than one)\n",
    "    def __select_bin(self, codes):\n",
    "        values, counts = np.unique(codes, return_counts=True)\n",
    "        max_count = np.max(counts)\n",
    "        most_common_values = values[counts == max_count]\n",
    "        return np.random.choice(most_common_values)\n",
    "\n",
    "    # Create the bins used for the index\n",
    "    def __create_bins(self, hashes, bins):\n",
    "        minval = np.min(hashes)\n",
    "        maxval = np.max(hashes)\n",
    "        return np.linspace(start=minval, stop=maxval, num=bins)\n",
    "\n",
    "    # Sort the hash codes into bins\n",
    "    def __hashes_to_bins(self, vectors, hash_codes, bins):\n",
    "        bin_index = np.digitize(hash_codes, bins)\n",
    "        bin_index = np.apply_along_axis(self.__select_bin, 1, bin_index)-1\n",
    "\n",
    "        bins_dict = dict()\n",
    "        for i in range(bins.shape[0]):\n",
    "            bins_dict[i] = vectors[bin_index == i]\n",
    "        return bins_dict\n",
    "\n",
    "    def __find_k_neighbours(self, target, K):\n",
    "        neighbours = self._binned_vectors[target]\n",
    "\n",
    "        if(neighbours.shape[0] < K):\n",
    "            low_bin = target-1\n",
    "            high_bin = target+1\n",
    "\n",
    "            while neighbours.shape[0] < K:\n",
    "                if low_bin >= 0:\n",
    "                    neighbours = np.concatenate((neighbours, self._binned_vectors[low_bin]), axis=0)\n",
    "                    low_bin -= 1\n",
    "                if high_bin < self._num_bins:\n",
    "                    neighbours = np.concatenate((neighbours, self._binned_vectors[high_bin]), axis=0)\n",
    "                    high_bin += 1\n",
    "                if low_bin < 0 and high_bin >= self._num_bins:\n",
    "                    break\n",
    "        \n",
    "        return neighbours[:K]\n",
    "\n",
    "    def search(self, vector, K=10):\n",
    "        hash_code = self._hasher.hash([vector])\n",
    "        bin_ids = np.digitize(hash_code, self._bins) - 1\n",
    "        bin_id = self.__select_bin(bin_ids)\n",
    "\n",
    "        candidate_vectors = self.__find_k_neighbours(bin_id, K)\n",
    "        l2_distances = np.sum((candidate_vectors[:,1:] - vector) ** 2, axis=1)\n",
    "        \n",
    "        sorted_indices = np.argsort(l2_distances)\n",
    "        result_indices = candidate_vectors[:,0][sorted_indices]\n",
    "        \n",
    "        return result_indices, l2_distances[sorted_indices]\n",
    "\n",
    "    # Save function for storing the index as a npz file\n",
    "    def save(self, path):\n",
    "        np.savez_compressed(\n",
    "            path,\n",
    "            properties = {\n",
    "                \"dim\": self._dim,\n",
    "                \"r\": self._r,\n",
    "                \"seed\": self._seed,\n",
    "                \"bins\": self._bins,\n",
    "                \"num_bins\": self._num_bins,\n",
    "                \"num_projections\": self._num_projections\n",
    "            },\n",
    "            binned_vectors = self._binned_vectors\n",
    "        )\n",
    "\n",
    "    # Loading the object from a npz file (to avoid having to rebuild it every time)\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        instance = cls.__new__(cls)\n",
    "        properties = data[\"properties\"].item()\n",
    "\n",
    "        instance._dim = properties[\"dim\"]\n",
    "        instance._r = properties[\"r\"]\n",
    "        instance._seed = properties[\"seed\"]\n",
    "        instance._num_projections = properties[\"num_projections\"]\n",
    "        instance._hasher = L2Hash(instance._dim, instance._r, instance._seed, instance._num_projections)\n",
    "\n",
    "        instance._bins = properties[\"bins\"]\n",
    "        instance._num_bins = properties[\"num_bins\"]\n",
    "        instance._binned_vectors = data[\"binned_vectors\"].item()\n",
    "        return instance\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LSHIndex ({', '.join([f'bin({i}) = {self._binned_vectors[i].shape[0]}' for i in self._binned_vectors if self._binned_vectors[i].shape[0] > 0])})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab76459-7d2f-4994-8271-077c95c1b9aa",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "- We created an LSHIndex class, it takes vectors, indices, and the number of projections and bins.\n",
    "- Projections are created, these are used to create hash_codes of each vector.\n",
    "- These hash_codes are then used to create bin boundaries, and bins are created.\n",
    "- The hash_codes are then put into these bins\n",
    "\n",
    "### Notes\n",
    "- Note that if the vectors are already normalized to 1, cosine similarity is just the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e11d2-0392-43da-922c-d4ba2cd1813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSHIndex (bin(32) = 1, bin(33) = 8, bin(34) = 26, bin(35) = 70, bin(36) = 184, bin(37) = 416, bin(38) = 1008, bin(39) = 2237, bin(40) = 3792, bin(41) = 6375, bin(42) = 10388, bin(43) = 15881, bin(44) = 21864, bin(45) = 29517, bin(46) = 37057, bin(47) = 44237, bin(48) = 49326, bin(49) = 51735, bin(50) = 53096, bin(51) = 51678, bin(52) = 48696, bin(53) = 42511, bin(54) = 35739, bin(55) = 28770, bin(56) = 20999, bin(57) = 15211, bin(58) = 10259, bin(59) = 6527, bin(60) = 3810, bin(61) = 1852, bin(62) = 1004, bin(63) = 446, bin(64) = 207, bin(65) = 69, bin(66) = 22, bin(67) = 15, bin(69) = 4)\n"
     ]
    }
   ],
   "source": [
    "index = LSHIndex(np.arange(embeddings.shape[0]), embeddings, 1, 100, 100)\n",
    "print(index)\n",
    "index.save('testindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b50202-4684-49f8-8c92-9b458c76d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "washington_titles = np.load('washington_idtitle', allow_pickle=True)[\"title\"]\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def find_documents(index, query, K=10):\n",
    "    query_vector = model.encode(query)\n",
    "    \n",
    "    indices = index.search(query_vector, K)[0]\n",
    "    return washington_titles.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205882fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "968     Has Obama taken Bush’s ‘preemption’ strategy to another level?\n",
       "656                              Obama closes the book on the 9/11 era\n",
       "77                                  Why Obama will (won’t) win in 2012\n",
       "1031                                                              None\n",
       "945                                    Hungary’s rush toward autocracy\n",
       "253         Marco Rubio has what Mitt Romney needs in a vice president\n",
       "371        Iran intensifies efforts to influence policy in Afghanistan\n",
       "450         Hosni Mubarak should be executed, Egyptian prosecutors say\n",
       "20         Argentine sports obsession spawns sports journalism schools\n",
       "638                                 A pledge that compromises our oath\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "find_documents(index, \"President USA\", 60)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 34s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92                                            Donald Trump sworn in as 45th president of the United States\n",
       "3173                                              Trump’s election threatens human rights around the world\n",
       "4760                                                                          A president unlike any other\n",
       "3594                               Strong presidencies may threaten democracy. Luckily, we don’t have one.\n",
       "4789               When Lincoln saved the union and freed the slaves, five ex-presidents tried to stop him\n",
       "4347                                                                   Trump makes America disdained again\n",
       "3009                        ‘Maximalist: America in the World from Truman to Obama’ by Stephen Sestanovich\n",
       "2507                                                Donald Trump and the expanding power of the presidency\n",
       "1828                  What did the Founders have in mind for the presidency? Here’s what you need to know.\n",
       "2533                                                Donald Trump and the expanding power of the presidency\n",
       "4480                                              Donald Trump and the repudiation of the political résumé\n",
       "4479                                              Donald Trump and the repudiation of the political résumé\n",
       "128                                                                   Cosmo’s guide to picking a president\n",
       "3021                                             Barack Obama Sworn In As U.S. President For A Second Term\n",
       "3022                                             Barack Obama Sworn In As U.S. President For A Second Term\n",
       "2649                                                                   A president’s potential to be great\n",
       "2919                         After Trump’s victory, the world is left to wonder: What happened to America?\n",
       "3829                                        Just imagine what kind of president we might elect after Trump\n",
       "4574                                                             ‘America first’ is becoming America alone\n",
       "2541                                            Trump’s favorite Middle East strongman comes to Washington\n",
       "533                            The president was never intended to be the most powerful part of government\n",
       "2299                                               The U.S. can’t afford to end its global leadership role\n",
       "4399                                                                Americans can choose better than Trump\n",
       "2238                                                             Trump is taking historic steps — backward\n",
       "1641                              Our next president must maintain America’s strong partnership with India\n",
       "739                                                 As it turns out America First does equal America Alone\n",
       "964                                               Pence likens Trump to one of his heroes: Teddy Roosevelt\n",
       "1854                                 Trump’s inaugural address was a radical break with American tradition\n",
       "4873                                               John Quincy Adams and the trait that broke a presidency\n",
       "3942                          Do campaign statements and tweets add up to a Trump foreign policy strategy?\n",
       "181                                      The greatest threat facing the United States is its own president\n",
       "1763                   America is swapping a surprisingly popular president for an unusually unpopular one\n",
       "253                                                              Clinton and Obama: Presidential parallels\n",
       "4323                                                                   Why do we have a president, anyway?\n",
       "927     Has the moral authority of the U.S. been eroded with Trump’s reaction to the violence in Virginia?\n",
       "2917                                                                Putin: Obama was right to skip summits\n",
       "2835                                              Memo to Trump: There can be only one president at a time\n",
       "1618                                           Career Coach: Your checklist for picking the next president\n",
       "1885                                                                   Barack Obama, disappointer in chief\n",
       "506                                                 Poll shows U.S. tumbling in world’s regard under Trump\n",
       "1898                                           Does separation of powers lead to a monarchical presidency?\n",
       "2851                                                                     America’s Golden Age of Stupidity\n",
       "617                                                 How Trump came to see himself as presidential material\n",
       "1068                                Obama’s last State of the Union will try to counter electorate’s anger\n",
       "2214                                                    ‘America first’ shouldn’t mean cutting foreign aid\n",
       "3722                                                                         Lamest. Administration. Ever.\n",
       "1864                                        The Daily 202: Donald Trump flips the script on foreign policy\n",
       "3251                  In Egypt, it’s past time for the Obama administration to use what power the U.S. has\n",
       "3895                                                          Obama is upending the role of the presidency\n",
       "2956                                                Donald Trump is elected president of the United States\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "find_documents(index, \"President USA\", 600000)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd0297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
