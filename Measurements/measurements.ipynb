{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for TREC topics and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSH import LSHIndex\n",
    "import numpy as np\n",
    "import timeit\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = np.load('../Data/washington_idtitle.npz', allow_pickle=True)['id']\n",
    "file_titles = np.load('../Data/washington_idtitle.npz', allow_pickle=True)['title']\n",
    "file_vectors = np.load('../Data/minilm_mean_vectors.npz')['vectors']\n",
    "\n",
    "# Get documentID by index\n",
    "def get_docid(i):\n",
    "    indices = i.astype(int)\n",
    "    return np.array(file_ids)[indices]\n",
    "\n",
    "# Get title by index\n",
    "def get_doctitle(i):\n",
    "    indices = i.astype(int)\n",
    "    return np.array(file_titles)[indices]\n",
    "\n",
    "# Get vector from documentID\n",
    "def vector_from_docid(docid):\n",
    "    vectors = file_vectors\n",
    "    index = np.where(np.array(file_ids) == docid)[0][0]\n",
    "    return vectors[index]\n",
    "    \n",
    "\n",
    "# Get nearest documents from another document ID\n",
    "def nearest_documents(index, docid, K=200):\n",
    "    query_vector = vector_from_docid(docid)\n",
    "    \n",
    "    # Retrieve more than needed, so the queried document can be removed\n",
    "    i, d = index.search(query_vector, K=K+20)\n",
    "    \n",
    "    # Remove the document itself from the results (since the evaluation will penalize it otherwise) by removing all documents with distance 0\n",
    "    filtered_index = np.where(d != 0)\n",
    "    i = i[filtered_index][:K]\n",
    "    d = d[filtered_index][:K]\n",
    "    \n",
    "    scores = 1 - (d - d.min()) / (d.max() - d.min())\n",
    "    return get_docid(i), scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for TREC RUN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics used for evaluation from a topics xml file\n",
    "def get_topics(path):\n",
    "    with open(path, \"r\") as input_file:\n",
    "        soup = BeautifulSoup(input_file.read(), \"xml\")\n",
    "        topics = soup.find_all('top')\n",
    "\n",
    "    return [(int(topic.find('num').text.split(': ')[1]), topic.find('docid').text) for topic in topics]\n",
    "\n",
    "# Create a RUN file given an index\n",
    "def create_RUN(index, name, K=200, num=0):\n",
    "    topics = get_topics('Trec/topics.backgroundlinking18.txt')\n",
    "    \n",
    "    with open(f'Trec/Runs/{name}_RUN.txt', 'w') as f:\n",
    "        for (num, docid) in topics:\n",
    "            nearest, scores = nearest_documents(index, docid, K=K)\n",
    "            i = 1\n",
    "            for doc, score in zip(nearest, scores):\n",
    "                f.write(f\"{num} Q0 {doc} {i} {score} {name}\\n\")\n",
    "                i += 1\n",
    "\n",
    "# Evaluate a RUN file using the given qrels\n",
    "def eval_RUN(qrels, runs, output):\n",
    "  try:\n",
    "    result = subprocess.run(['wsl',\n",
    "                             './Trec/trec_eval',\n",
    "                             '-c', '-M1000',\n",
    "                             '-m', 'map',\n",
    "                             '-m', 'P.10',\n",
    "                             '-m', 'ndcg_cut.5',\n",
    "                             '-m', 'recall.100',\n",
    "                             qrels,\n",
    "                             runs],\n",
    "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(result.stderr)\n",
    "    with open(output, 'w') as f:\n",
    "      f.write(result.stdout)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Retrieve the NDCG@5 score from an eval file\n",
    "def retrieve_NDCG(eval_file):\n",
    "    df = pd.read_csv(eval_file, sep='\\s+', header=None, names=[\"Metric\", \"All\", \"Value\"])\n",
    "    return df.loc[df[\"Metric\"] == \"ndcg_cut_5\", \"Value\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our measurement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure average timer per query for reps rounds\n",
    "def time_per_query(index, query, reps=50, K=200):\n",
    "    return timeit.timeit(f\"index.search(query, K)\", globals={\"index\": index, \"query\": query, \"K\": K}, number=reps)/reps\n",
    "\n",
    "# Measure index performance\n",
    "def measure_index(index, queries, name, K=50):\n",
    "    print(f\"started...\", end='')\n",
    "    res = dict()\n",
    "    \n",
    "    times = [time_per_query(index, query, K=K) for query in queries]\n",
    "    print(\"got times...\", end='')\n",
    "    \n",
    "    res['num_queries'] = len(queries)\n",
    "    res['min_time'] = min(times)\n",
    "    res['max_time'] = max(times)\n",
    "    res['mean_time'] = sum(times)/len(queries)\n",
    "    \n",
    "    create_RUN(index, name, K=K)\n",
    "    eval_RUN('Trec/qrels.backgroundlinking18.txt', f'Trec/Runs/{name}_RUN.txt', f'Trec/Evals/{name}.txt')\n",
    "    res['ndcg_5'] = retrieve_NDCG(f'Trec/Evals/{name}.txt')\n",
    "    print(\"done!\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The actual measurements\n",
    "\n",
    "To know the amount of bins we want, we looked into the rice rule and the freedman-diaconis rule, however both are used for histogram binning.\n",
    "\n",
    "Measure:\n",
    "- 5 lines for each `r` in `[0.5, 1.3, 2, 5, 10]`\n",
    "- x-axis: `#bins`\n",
    "- y-axis: `NDCG@5` or `mean search time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('../Data/minilm_mean_vectors.npz')['vectors']\n",
    "# Sample 20 vectors as the queries\n",
    "queries = embeddings[np.random.choice(np.arange(embeddings.shape[0]), 20)]\n",
    "n = embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r0.5_x6 already measured!\n",
      "r0.5_x7 already measured!\n",
      "r0.5_x8 already measured!\n",
      "r0.5_x9 already measured!\n",
      "r0.5_x10 already measured!\n",
      "r0.5_x11 already measured!\n",
      "r0.8_x6 already measured!\n",
      "r0.8_x7 already measured!\n",
      "r0.8_x8 already measured!\n",
      "r0.8_x9 already measured!\n",
      "r0.8_x10 already measured!\n",
      "r0.8_x11 already measured!\n",
      "r1.3_x6 already measured!\n",
      "r1.3_x7 already measured!\n",
      "r1.3_x8 already measured!\n",
      "r1.3_x9 already measured!\n",
      "r1.3_x10 already measured!\n",
      "r1.3_x11 already measured!\n",
      "r2.0_x6 already measured!\n",
      "r2.0_x7 already measured!\n",
      "r2.0_x8 already measured!\n",
      "r2.0_x9 already measured!\n",
      "r2.0_x10 already measured!\n",
      "r2.0_x11 already measured!\n",
      "r5.0_x6 already measured!\n",
      "r5.0_x7 already measured!\n",
      "r5.0_x8 already measured!\n",
      "r5.0_x9 already measured!\n",
      "r5.0_x10 already measured!\n",
      "r5.0_x11 already measured!\n",
      "total_res stored\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "rs = np.array([0.5, 0.8, 1.3, 2, 5])\n",
    "xs = np.arange(6, 12)\n",
    "\n",
    "file = open(\"parameter_optimisation.pkl\",'rb')\n",
    "total_res = pickle.load(file)\n",
    "\n",
    "for r in rs:\n",
    "    for x in xs:\n",
    "        name = f'r{r}_x{x}'\n",
    "        \n",
    "        if name in total_res:\n",
    "            print(f\"{name} already measured!\")\n",
    "            continue\n",
    "        \n",
    "        index = LSHIndex(embeddings.shape[1], r, x, 100)\n",
    "        index.add(np.arange(embeddings.shape[0]), embeddings)\n",
    "        \n",
    "        print(f\"Measuring r{r} x{x}: \", end='')\n",
    "\n",
    "        dic = measure_index(index, queries, name)\n",
    "        total_res[name] = dic\n",
    "        print(total_res)\n",
    "        \n",
    "with open('parameter_optimisation.pkl', 'wb') as f:\n",
    "    pickle.dump(total_res, f)\n",
    "    print('total_res stored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements\n",
    "\n",
    "- We measure min, max, and mean time over 20 queries sampled from the source vectors.\n",
    "This is because if the distribution over bins is good, we will have min and max close to the mean.\n",
    "So when we select a value for r, we need to check these values for a good distribution over the bins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r0.5_x6': {'num_queries': 20,\n",
       "  'min_time': 0.016373948000837118,\n",
       "  'max_time': 0.28149472599849107,\n",
       "  'mean_time': 0.09390679139981514,\n",
       "  'ndcg_5': 0.1668},\n",
       " 'r0.5_x7': {'num_queries': 20,\n",
       "  'min_time': 0.005667737999465317,\n",
       "  'max_time': 0.08985833599930629,\n",
       "  'mean_time': 0.03328375279984903,\n",
       "  'ndcg_5': 0.1247},\n",
       " 'r0.5_x8': {'num_queries': 20,\n",
       "  'min_time': 0.0019153680000454187,\n",
       "  'max_time': 0.057285086000338194,\n",
       "  'mean_time': 0.01759525760007091,\n",
       "  'ndcg_5': 0.0918},\n",
       " 'r0.5_x9': {'num_queries': 20,\n",
       "  'min_time': 0.0022914760001003743,\n",
       "  'max_time': 0.03249052600003779,\n",
       "  'mean_time': 0.008412997000035831,\n",
       "  'ndcg_5': 0.0912},\n",
       " 'r0.5_x10': {'num_queries': 20,\n",
       "  'min_time': 0.00258402599953115,\n",
       "  'max_time': 0.02361889399820939,\n",
       "  'mean_time': 0.005662937099812553,\n",
       "  'ndcg_5': 0.0553},\n",
       " 'r0.5_x11': {'num_queries': 20,\n",
       "  'min_time': 0.0052177079999819395,\n",
       "  'max_time': 0.013893606001511216,\n",
       "  'mean_time': 0.007432482300093396,\n",
       "  'ndcg_5': 0.0676},\n",
       " 'r1.3_x6': {'num_queries': 20,\n",
       "  'min_time': 0.021163711999543013,\n",
       "  'max_time': 1.6496113559999503,\n",
       "  'mean_time': 1.0360106591000224,\n",
       "  'ndcg_5': 0.241},\n",
       " 'r1.3_x7': {'num_queries': 20,\n",
       "  'min_time': 0.004005593999754638,\n",
       "  'max_time': 0.11137277799891308,\n",
       "  'mean_time': 0.043621114899870014,\n",
       "  'ndcg_5': 0.1158},\n",
       " 'r1.3_x8': {'num_queries': 20,\n",
       "  'min_time': 0.0021507419995032253,\n",
       "  'max_time': 0.34568243799963966,\n",
       "  'mean_time': 0.09350147540017495,\n",
       "  'ndcg_5': 0.1635},\n",
       " 'r1.3_x9': {'num_queries': 20,\n",
       "  'min_time': 0.002822072000708431,\n",
       "  'max_time': 0.03473320199875161,\n",
       "  'mean_time': 0.009811328200274147,\n",
       "  'ndcg_5': 0.0799},\n",
       " 'r1.3_x10': {'num_queries': 20,\n",
       "  'min_time': 0.003697239998728037,\n",
       "  'max_time': 0.02640077000018209,\n",
       "  'mean_time': 0.006630676600150764,\n",
       "  'ndcg_5': 0.0779},\n",
       " 'r1.3_x11': {'num_queries': 20,\n",
       "  'min_time': 0.0057825980009511115,\n",
       "  'max_time': 0.021545866001397372,\n",
       "  'mean_time': 0.009598102900432424,\n",
       "  'ndcg_5': 0.0813},\n",
       " 'r2.0_x6': {'num_queries': 20,\n",
       "  'min_time': 0.035326290000230076,\n",
       "  'max_time': 2.7094734680000694,\n",
       "  'mean_time': 2.0048208017998843,\n",
       "  'ndcg_5': 0.2633},\n",
       " 'r2.0_x7': {'num_queries': 20,\n",
       "  'min_time': 0.001966222000773996,\n",
       "  'max_time': 0.10816774799954146,\n",
       "  'mean_time': 0.04467333170003258,\n",
       "  'ndcg_5': 0.1079},\n",
       " 'r2.0_x8': {'num_queries': 20,\n",
       "  'min_time': 0.01944328400073573,\n",
       "  'max_time': 0.8473572460003197,\n",
       "  'mean_time': 0.3101999452997698,\n",
       "  'ndcg_5': 0.1918},\n",
       " 'r2.0_x9': {'num_queries': 20,\n",
       "  'min_time': 0.002360503999516368,\n",
       "  'max_time': 0.04345182199962437,\n",
       "  'mean_time': 0.01302346959989518,\n",
       "  'ndcg_5': 0.0835},\n",
       " 'r2.0_x10': {'num_queries': 20,\n",
       "  'min_time': 0.0034211879991926254,\n",
       "  'max_time': 0.0262784059997648,\n",
       "  'mean_time': 0.006528779000043869,\n",
       "  'ndcg_5': 0.0775},\n",
       " 'r2.0_x11': {'num_queries': 20,\n",
       "  'min_time': 0.005910380000714212,\n",
       "  'max_time': 0.05946795399999246,\n",
       "  'mean_time': 0.0179219168999698,\n",
       "  'ndcg_5': 0.1199},\n",
       " 'r5.0_x6': {'num_queries': 20,\n",
       "  'min_time': 2.339781782000791,\n",
       "  'max_time': 3.1616945619997567,\n",
       "  'mean_time': 2.988347264800221,\n",
       "  'ndcg_5': 0.2916},\n",
       " 'r5.0_x7': {'num_queries': 20,\n",
       "  'min_time': 0.003105926001444459,\n",
       "  'max_time': 0.601286003999412,\n",
       "  'mean_time': 0.19671055029984563,\n",
       "  'ndcg_5': 0.1694},\n",
       " 'r5.0_x8': {'num_queries': 20,\n",
       "  'min_time': 0.032794327998999506,\n",
       "  'max_time': 2.300055668000132,\n",
       "  'mean_time': 1.9972438941000727,\n",
       "  'ndcg_5': 0.2784},\n",
       " 'r5.0_x9': {'num_queries': 20,\n",
       "  'min_time': 0.0012328280019573868,\n",
       "  'max_time': 0.19086913200095296,\n",
       "  'mean_time': 0.0451306346001802,\n",
       "  'ndcg_5': 0.145},\n",
       " 'r5.0_x10': {'num_queries': 20,\n",
       "  'min_time': 0.002032735999673605,\n",
       "  'max_time': 0.025520077999681236,\n",
       "  'mean_time': 0.006587304299930111,\n",
       "  'ndcg_5': 0.0864},\n",
       " 'r5.0_x11': {'num_queries': 20,\n",
       "  'min_time': 0.021002263999544083,\n",
       "  'max_time': 1.2377288820012473,\n",
       "  'mean_time': 0.5285475488002411,\n",
       "  'ndcg_5': 0.2052},\n",
       " 'r0.8_x6': {'num_queries': 20,\n",
       "  'min_time': 0.007048337999731302,\n",
       "  'max_time': 0.5103299360000528,\n",
       "  'mean_time': 0.17036928490025458,\n",
       "  'ndcg_5': 0.1982},\n",
       " 'r0.8_x7': {'num_queries': 20,\n",
       "  'min_time': 0.0026282460009679196,\n",
       "  'max_time': 0.07518617400201037,\n",
       "  'mean_time': 0.02708809810015373,\n",
       "  'ndcg_5': 0.1107},\n",
       " 'r0.8_x8': {'num_queries': 20,\n",
       "  'min_time': 0.0009567980002611876,\n",
       "  'max_time': 0.09696505999891088,\n",
       "  'mean_time': 0.02622698349982966,\n",
       "  'ndcg_5': 0.1177},\n",
       " 'r0.8_x9': {'num_queries': 20,\n",
       "  'min_time': 0.0015800540009513497,\n",
       "  'max_time': 0.023031466000247747,\n",
       "  'mean_time': 0.005923718699836172,\n",
       "  'ndcg_5': 0.0817},\n",
       " 'r0.8_x10': {'num_queries': 20,\n",
       "  'min_time': 0.0020531419990584255,\n",
       "  'max_time': 0.018352608000859617,\n",
       "  'mean_time': 0.004025806599878707,\n",
       "  'ndcg_5': 0.0627},\n",
       " 'r0.8_x11': {'num_queries': 20,\n",
       "  'min_time': 0.0041557920002378524,\n",
       "  'max_time': 0.008186901998706163,\n",
       "  'mean_time': 0.005461204700288362,\n",
       "  'ndcg_5': 0.0617}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
