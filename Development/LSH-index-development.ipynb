{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08489670-0332-4612-a59b-311000828eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327d70a8-af6c-494b-9e05-2edf85e679e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended: 7\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load('../Data/minilm_mean_vectors.npz')['vectors']\n",
    "n = embeddings.shape[0]\n",
    "\n",
    "print(f\"Recommended: {math.ceil(math.log2(pow(2*n, 1/3)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eae8d47-effe-43a0-9d50-73fe2410e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Hash:\n",
    "    def __init__(self, dim, r, nbits, seed=1):\n",
    "        self.seed = seed\n",
    "        self.nbits = nbits\n",
    "        \n",
    "        gen = np.random.RandomState(seed)\n",
    "        self.a = gen.normal(0, 1, (nbits, dim))\n",
    "        self.b = gen.uniform(0.0, r)\n",
    "        self.r = r\n",
    "\n",
    "    def hash(self, vectors):\n",
    "        hash_values = (np.dot(vectors, self.a.T) + self.b) / self.r\n",
    "        hash_binary = (hash_values >= 0).astype(int)\n",
    "        return np.apply_along_axis(lambda row: ''.join(row.astype(str)), axis=1, arr=hash_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b2e819-6400-4a12-8000-bb491906368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHIndex:\n",
    "    def __init__(self, dim, r=5, nbits=5, seed=1):\n",
    "        # Store the vector dimension\n",
    "        self._dim = dim\n",
    "\n",
    "        # Created hash codes by applying our projections\n",
    "        self._hasher = L2Hash(self._dim, r, nbits, seed)\n",
    "        self._r = r\n",
    "        self._nbits = nbits\n",
    "        self._seed = seed\n",
    "        self._binned_vectors = dict()\n",
    "\n",
    "    # Sort the hash codes into bins\n",
    "    def __hashes_to_bins(self, vectors, hash_codes):\n",
    "        bins_dict = dict()\n",
    "        unique_bins = np.unique(hash_codes)\n",
    "        \n",
    "        for cur_bin in unique_bins:\n",
    "            bins_dict[cur_bin] = vectors[hash_codes == cur_bin]\n",
    "        \n",
    "        return bins_dict\n",
    "\n",
    "    # Find the closest bins to a hash code by Hamming distance\n",
    "    def __closest_bins(self, hash_code):\n",
    "        bins = np.array(list(self._binned_vectors.keys()))\n",
    "        \n",
    "        # Calculate Hamming distance\n",
    "        distances = np.array([sum(c1 != c2 for c1, c2 in zip(hash_code, bin)) for bin in bins])\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        return np.array(bins)[sorted_indices]\n",
    "\n",
    "    # Find the K nearest neighbours for a given bin\n",
    "    def __find_k_neighbours(self, target, K):\n",
    "        closest_bins = self.__closest_bins(target)\n",
    "        neighbours = self._binned_vectors[closest_bins[0]]\n",
    "\n",
    "        index = 1\n",
    "        while (neighbours.shape[0] < K and index < len(closest_bins)):\n",
    "            neighbours = np.concatenate((neighbours, self._binned_vectors[closest_bins[index]]), axis=0)\n",
    "            index += 1\n",
    "        \n",
    "        return neighbours\n",
    "\n",
    "    # Add function which adds vectors with their given indices to the index\n",
    "    def add(self, indices, vectors):\n",
    "        if (self._dim != vectors.shape[1]):\n",
    "            raise Exception(f\"Dimension mismatch: Index ({self._dim}) and vectors ({vectors.shape[1]})\")\n",
    "        \n",
    "        # Add indexes into vectors (such that we can find the original index after binning)\n",
    "        indexed_vectors = np.hstack((indices[:, np.newaxis], vectors))\n",
    "        hash_codes = self._hasher.hash(vectors)\n",
    "        self._binned_vectors = self.__hashes_to_bins(indexed_vectors, hash_codes)\n",
    "\n",
    "    # Search function which accepts a vector and a K value (number of results requested)\n",
    "    def search(self, vector, K=10):\n",
    "        # Hash the query to its code\n",
    "        hash_code = self._hasher.hash([vector])[0]\n",
    "        \n",
    "        # Find the nearest bins to satisfy K results\n",
    "        candidate_vectors = self.__find_k_neighbours(hash_code, K)\n",
    "    \n",
    "        # Calculate Euclidean distance\n",
    "        distances = np.sum((candidate_vectors[:,1:] - vector) ** 2, axis=1)\n",
    "        sorted_indices = np.argsort(distances)[:K]\n",
    "        \n",
    "        # Return the indices of the ranked results\n",
    "        result_indices = candidate_vectors[:,0][sorted_indices]\n",
    "        return result_indices, distances[sorted_indices]\n",
    "\n",
    "    # Save function for storing the index as a npz file\n",
    "    def save(self, path):\n",
    "        np.savez(\n",
    "            path,\n",
    "            properties = {\n",
    "                \"dim\": self._dim,\n",
    "                \"r\": self._r,\n",
    "                \"seed\": self._seed,\n",
    "                \"nbits\": self._nbits\n",
    "            },\n",
    "            binned_vectors = self._binned_vectors\n",
    "        )\n",
    "\n",
    "    # Loading the object from a npz file (to avoid having to rebuild it each time, and to be able to see index size after quantizing)\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        instance = cls.__new__(cls)\n",
    "        properties = data[\"properties\"].item()\n",
    "\n",
    "        instance._dim = properties[\"dim\"]\n",
    "        instance._r = properties[\"r\"]\n",
    "        instance._nbits = properties[\"nbits\"]\n",
    "        instance._seed = properties[\"seed\"]\n",
    "        instance._hasher = L2Hash(instance._dim, instance._r, instance._seed, instance._nbits)\n",
    "\n",
    "        instance._binned_vectors = data[\"binned_vectors\"].item()\n",
    "        return instance\n",
    "\n",
    "    # toString method, showcases the distribution of vectors over the bins\n",
    "    def __str__(self):\n",
    "        unique_bins = self._binned_vectors.keys()\n",
    "        return f\"LSHIndex ({', '.join([f'bin({unique_bin}) = {len(self._binned_vectors[unique_bin])}' for unique_bin in unique_bins])})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab76459-7d2f-4994-8271-077c95c1b9aa",
   "metadata": {},
   "source": [
    "## Steps (original approach)\n",
    "\n",
    "- We created an LSHIndex class, it takes vectors, indices, and the number of projections and bins.\n",
    "- Projections are created, these are used to create hash_codes of each vector.\n",
    "- These hash_codes are then used to create bin boundaries, and bins are created.\n",
    "- The hash_codes are then put into these bins\n",
    "\n",
    "## Steps (p-Stable)\n",
    "\n",
    "- We didn't have a proper hash function, we just used random projection vectors. \n",
    "We implemented the hashing function from p-Stable distributions article.\n",
    "- We still binned by taking the lowest and highest hash code as the bin boundaries. \n",
    "This now resulted in a Gaussian distribution of vectors over bins, obviously.\n",
    "- We then tried to find a way to distribute the vectors evenly.\n",
    "\n",
    "- We then looked into the binning strategy (which we misunderstood). \n",
    "The resulting hashcodes can be mapped to either 1 or 0, creating a bit. \n",
    "We then know a bin a vector belongs to by creating a hash table.\n",
    "\n",
    "### Notes\n",
    "- We used euclidean distance\n",
    "\n",
    "## Redo\n",
    "\n",
    "- We try out rice rule\n",
    "- Freedman-diaconis\n",
    "\n",
    "- Also try for multiple r values\n",
    "- r = 1, 1.5, 5, 10\n",
    "- try both rules\n",
    "\n",
    "## Measure results\n",
    "\n",
    "- try both rules for getting bins\n",
    "- 4 different r values (more?)\n",
    "\n",
    "Implement best rule in index\n",
    "Apply quantizers on data (query?)\n",
    "Measure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0e11d2-0392-43da-922c-d4ba2cd1813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSHIndex (bin(0000000) = 1244, bin(0000001) = 1012, bin(0000010) = 2101, bin(0000011) = 8106, bin(0000100) = 1917, bin(0000101) = 1277, bin(0000110) = 3008, bin(0000111) = 10598, bin(0001000) = 1003, bin(0001001) = 909, bin(0001010) = 1574, bin(0001011) = 5137, bin(0001100) = 1316, bin(0001101) = 1044, bin(0001110) = 1714, bin(0001111) = 5334, bin(0010000) = 845, bin(0010001) = 929, bin(0010010) = 2648, bin(0010011) = 11247, bin(0010100) = 1638, bin(0010101) = 2110, bin(0010110) = 3976, bin(0010111) = 20143, bin(0011000) = 1519, bin(0011001) = 1200, bin(0011010) = 2833, bin(0011011) = 8647, bin(0011100) = 2675, bin(0011101) = 2186, bin(0011110) = 5065, bin(0011111) = 14708, bin(0100000) = 1520, bin(0100001) = 1094, bin(0100010) = 1406, bin(0100011) = 3848, bin(0100100) = 4357, bin(0100101) = 2031, bin(0100110) = 2735, bin(0100111) = 8073, bin(0101000) = 1192, bin(0101001) = 784, bin(0101010) = 1226, bin(0101011) = 2867, bin(0101100) = 3272, bin(0101101) = 1492, bin(0101110) = 1841, bin(0101111) = 4541, bin(0110000) = 1427, bin(0110001) = 1034, bin(0110010) = 2512, bin(0110011) = 6051, bin(0110100) = 5117, bin(0110101) = 4118, bin(0110110) = 6214, bin(0110111) = 20352, bin(0111000) = 2169, bin(0111001) = 1372, bin(0111010) = 3866, bin(0111011) = 7520, bin(0111100) = 6133, bin(0111101) = 3898, bin(0111110) = 8922, bin(0111111) = 19119, bin(1000000) = 1731, bin(1000001) = 1433, bin(1000010) = 4034, bin(1000011) = 12520, bin(1000100) = 3003, bin(1000101) = 2909, bin(1000110) = 5292, bin(1000111) = 15598, bin(1001000) = 2163, bin(1001001) = 1726, bin(1001010) = 4302, bin(1001011) = 10546, bin(1001100) = 3488, bin(1001101) = 2213, bin(1001110) = 5677, bin(1001111) = 11420, bin(1010000) = 930, bin(1010001) = 1145, bin(1010010) = 2980, bin(1010011) = 10731, bin(1010100) = 2026, bin(1010101) = 2216, bin(1010110) = 5660, bin(1010111) = 17079, bin(1011000) = 2102, bin(1011001) = 1551, bin(1011010) = 4626, bin(1011011) = 9602, bin(1011100) = 3629, bin(1011101) = 3017, bin(1011110) = 7684, bin(1011111) = 15086, bin(1100000) = 1667, bin(1100001) = 827, bin(1100010) = 1943, bin(1100011) = 4304, bin(1100100) = 4383, bin(1100101) = 1665, bin(1100110) = 4154, bin(1100111) = 8020, bin(1101000) = 2180, bin(1101001) = 990, bin(1101010) = 2934, bin(1101011) = 5286, bin(1101100) = 4958, bin(1101101) = 2263, bin(1101110) = 4787, bin(1101111) = 7663, bin(1110000) = 1232, bin(1110001) = 784, bin(1110010) = 1922, bin(1110011) = 4192, bin(1110100) = 4347, bin(1110101) = 2545, bin(1110110) = 6575, bin(1110111) = 13325, bin(1111000) = 2909, bin(1111001) = 1376, bin(1111010) = 3919, bin(1111011) = 5719, bin(1111100) = 9193, bin(1111101) = 4046, bin(1111110) = 9827, bin(1111111) = 15217)\n"
     ]
    }
   ],
   "source": [
    "index = LSHIndex(embeddings.shape[1], 1.3, 7, 100)\n",
    "index.add(np.arange(embeddings.shape[0]), embeddings)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b50202-4684-49f8-8c92-9b458c76d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "washington_titles = np.load('../Data/washington_idtitle.npz', allow_pickle=True)[\"title\"]\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def find_documents(index, query, K=10):\n",
    "    query_vector = model.encode(query)\n",
    "    \n",
    "    indices = index.search(query_vector, K)[0]\n",
    "    return washington_titles.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7efec0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292        Two-thirds of Trump voters viewed the election as America’s last chance\n",
       "1291        Two-thirds of Trump voters viewed the election as America’s last chance\n",
       "468              Were the polls way off in 2016? A new report offers a mixed answer\n",
       "3410    The 2016 national polls are looking less wrong after final election tallies\n",
       "4424                                          Election Essentials: 2016 voter guide\n",
       "                                           ...                                     \n",
       "3334      Election recount will take place in Wisconsin, after Stein files petition\n",
       "3333      Election recount will take place in Wisconsin, after Stein files petition\n",
       "3332      Election recount will take place in Wisconsin, after Stein files petition\n",
       "3328      Election recount will take place in Wisconsin, after Stein files petition\n",
       "3591      Election recount will take place in Wisconsin, after Stein files petition\n",
       "Name: title, Length: 200, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_documents(index, \"American election 2016\", K=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b25c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
